{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhaltliche Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kennst nun die Statistik der Metadaten und einige Korrelationen, die dich zuversichtlich stimmen, dass der Transport-Flair des Technology-Subreddit gut geeignet für deine Analyse ist.\n",
    "\n",
    "Allerdings musst du noch die inhaltliche Seite überprüfen. So wäre es z.B. möglich, dass das Reddit voller Spam-Nachrichten ist oder die Diskussion trotz des Namens in eine völlig andere Richtung gehen. Dazu musst du die Texte analysieren.\n",
    "\n",
    "Du lädst zunächst die Title und Texte aus der Datenbank ein: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(\"transport-all-comments.csv.gz\", parse_dates=[\"created_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die einzelnen Wörter zu zählen, ist der `Counter` aus dem `collections`-Paket von Python optimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du betrachtest zunächst die Titel und müssen diese nun in Wörter zerlegen. [Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung) ist ein nicht-triviales Problem, das man normalerweise mit spezieller Software wie etwas [spaCy](https://spacy.io) lösen sollte. Das sparst du dir allerdings hier und nutzt einen einfache `regex`-Tokenizer, weil du sonst sehr lange auf die Ergebnisse waren müsstest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from tqdm.auto import tqdm\n",
    "title_counter = Counter([w.lower() for t in tqdm(posts[\"text\"]) for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `title_counter` verfügt über eine `most_common`-Funktion, mit der du dir die häufigsten Wörter ausgeben lassen könntest. Stattdessen nutzt du Wordclouds, die dir eine intuitive Visualisierung bieten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "wc.generate_from_frequencies(title_counter)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider kann man außer sehr allgemeinen Wörtern nicht viel erkennen. Wir müssen die Ergebnisse filtern und die sog. *Stoppworte* eliminieren. Zum Glück gibt es dazu fertige Listen, die wir hier noch etwas ergänzen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "for w in \"nan removed deleted post message account moderators http https www youtube com \\\n",
    "          watch gt look looks feel test know think go going submission link apologize \\\n",
    "          inconvenience don want automatically based buy compose good image karma like \\\n",
    "          lot need people self shit sound sounds spam submitting subreddit things \\\n",
    "          video way years time days doesn en fuck money org read reddit review \\\n",
    "          right said says subreddit subreddits sure thank try use videos wiki \\\n",
    "          wikipedia work ll thing point ve actually wait hello new amp better \\\n",
    "          isn yeah probably pretty yes didn pay long posts commenting portion \\\n",
    "          contribute questions unfortunately allowed submissions gifs pics sidebar\".split(\" \"):\n",
    "    stopwords.add(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du verwendest diese Liste und lässt einbuchstabige Wörter auch gleich weg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counter = Counter([w for t in tqdm(posts[\"text\"].str.lower())\n",
    "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
    "                               if (w not in stopwords) and (len(w) > 1)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wordcloud kann wieder genauso erzeugt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "wc.generate_from_frequencies(title_counter)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sieht schon sehr gut aus und passt genau zum Thema. Wunderbar, das bedeutet, dass du die richtige Datenmenge ausgewählt hast und auch unsere Klassifikation gut funktioniert hat.\n",
    "\n",
    "Analysiere zum Vergleich noch den Text der Toplevel-Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counter = Counter([w for t in tqdm(posts[posts[\"parent_id\"].isna()][\"text\"].str.lower()) \n",
    "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
    "                               if (w not in stopwords) and (len(w) > 1)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "wc.generate_from_frequencies(text_counter)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch das passt prima und sieht sehr ähnlich aus wie die Ergebnisse inkl. der Kommentare. In anderen Worten bedeutet das, dass die Kommentare gut zu den Posts passen - wunderbar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher hast du die inhaltlichen Aspekte der Posts nur durch Zählen der Wörter berücksichtigt. Allerdings interessieren dich auch Nischen-Themen, die du mithilfe sog. [Topic Modelle](https://en.wikipedia.org/wiki/Topic_model) ermitteln kannst.\n",
    "\n",
    "Hierbei handelt es sich um ein unüberwachtes Machine Learning-Verfahren zur Aufdeckung der latenten Struktur großer Datenmengen.\n",
    "\n",
    "Am häufigsten wird für Topic Models die sog. [LDA-Methode](https://de.wikipedia.org/wiki/Latent_Dirichlet_Allocation) eingesetzt, die mit stochastischem Sampling funktioniert. Da die Berechnung sehr lange benötigt und es sich in vielen Projekten gezeigt hat, dass die Ergebnisse des NMF-Algorithmus oft (mindestens) genauso gut sind, nutzt du diesen.\n",
    "\n",
    "Dafür werden die Texte im ersten Schritt mit TD/IDF vektorisiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(posts['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du das Topic Model instanziieren und berechnen lassen. Bei (fast) allen Topic Models musst du die Anzahl der Topics vorgeben. Es gibt bestimmte Metriken wie Perplexität oder Kohärenz, mit denen sich die Güte des Modells bestimmen lässt. In diesem Fall arbeitest du einfach mit 10 Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Berechnung dauert normalerweise keine Minute, jetzt kannst du die Daten visualisieren. Dafür nutzt du eine kleine Hilfsfunktion, die über die Topics iteriert und Wordclouds als Ergebnisse darstellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud_topic_model_summary(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        freq = {}\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            freq[feature_names[i].replace(\" \", \"_\")] = topic[i]\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(freq)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst di nun die Wordclouds für die 10 Topics aus dem Topic Model ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud_topic_model_summary(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out(), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plötzlich kannst du auch Nischenthemen erkennen, die dir vorher verborgen waren. Das ist sehr praktisch, um Ideen für Trends zu identifizieren. Hiermit kannst du außerdem erkennen, ob bestimmte Wörter möglicherweise noch eliminiert werden müssen (wie z.B. `deleted post`, das sich deswegen auch in den Stopwords findet).\n",
    "\n",
    "Durch die Geschwindigkeit, mit der ein NMF-Topic Model berechnet werden kann, bietet sich diese Methode auch zur Qualitätssicherung an."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
